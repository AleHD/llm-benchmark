[llama_3_8b]
bos_token_id = 128000
eos_token_id = 128001
hidden_size = 4096
intermediate_size = 14336
max_position_embeddings = 8192
num_attention_heads = 32
num_hidden_layers = 32
num_key_value_heads = 8
vocab_size = 128256
